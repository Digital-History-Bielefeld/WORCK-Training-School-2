{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmd4W3oovh_c"
   },
   "source": [
    "## Visualisation with Matplotlib\n",
    "\n",
    "In this tutorial, you will learn about data visualization using Matplotlib. Visualization plays a crucial role in understanding and interpreting data, and one common task is visualizing word frequencies in a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTqW7E-c-C8U"
   },
   "source": [
    "## Why visualise word frequencies?\n",
    "\n",
    "Understanding word frequencies can provide valuable insights into the content of a text. Whether you are analyzing literary works, exploring patterns in historical documents, or studying any other corpus, visualizing word frequencies can help highlight key themes, identify significant terms, and uncover patterns in language usage.\n",
    "\n",
    "Matplotlib, a powerful and widely-used plotting library in Python, provides a straightforward way to create visualizations that enhance our understanding of data. By employing Matplotlib, we can transform word frequency data into clear and informative visual representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSt8xCZN-ykl"
   },
   "source": [
    "## Defining the input text\n",
    "\n",
    "First, we need to save the text for which we want to analyse the word frequencies in a variable so we can easily access it. Since we usually calculate word frequencies for longer texts, it makes sense to read the input directly from the file.\n",
    "\n",
    "For this, we need to specify the path to our file. You can use the extract from Alice in Wonderland by Lewis Carroll you will find in the location `'panel-7/data/alice_in_wonderland.txt'`. Alternatively, you can use a `.txt` input file you are interested in analysing by uploading it to the same location and specifying the path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1707563331947,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "rSSu1jxgBClw",
    "outputId": "355b7b2b-d76c-4313-968c-78755b2083c4"
   },
   "outputs": [],
   "source": [
    "# TODO: Specify the file path\n",
    "\n",
    "\n",
    "# Open the file in read mode ('r')\n",
    "with open(file_path, 'r') as file:\n",
    "    # Read the entire content of the file\n",
    "    input = file.read()\n",
    "\n",
    "# TODO: print the content of the input variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feYRV18M-SQe"
   },
   "source": [
    "## Pre-processing the Input for Visualisation\n",
    "\n",
    "Preprocessing is crucial for calculating word frequencies. Tasks like tokenization and removing stop words ensure accurate and meaningful word frequency data.\n",
    "\n",
    "In the following, it's up to you: You can perform all the pre-processing steps manually to see how your input changes step-by-step. This involves more programming :) Or you can do it the easy way and use the text processing pipeline provided by the spaCy library. A third equally convenient way to do it is to use the `CountVectorizer` provided by `sklearn` which performs all the steps for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you get started ...\n",
    "\n",
    "Please run the code below. It will import some necessary libaries. This might take some time ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the libraries listed in thre requirements.txt file\n",
    "%pip install -r ../.devcontainer/python-3.12/requirements.txt --upgrade-strategy only-if-needed\n",
    "\n",
    "# install spacy model en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "259zDQ4Ma_Pj"
   },
   "source": [
    "## Option A: Pre-processing step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhRdVSvSBJDT"
   },
   "source": [
    "**Step 1: Pre-processing - Setting the text to lower case**\n",
    "\n",
    "In order to be able to compare words, we have to make sure that words written with capital and lower-case letters are counted as the same word. Apply the `lower()` function to the text so we only have words with lower-case letters. Print your results afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1707563875973,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "hzqQjxw7qwjo",
    "outputId": "c79c8b2f-b74b-48b6-8ec1-28149cfcef2b"
   },
   "outputs": [],
   "source": [
    "# TODO: Set the complete text to lower-case and save it in a new variable text_lower_case\n",
    "\n",
    "\n",
    "# TODO: Print the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPb0G6YuHAYX"
   },
   "source": [
    "**Step 2: Pre-processing - Tokenization**\n",
    "\n",
    "Since we want to count the words in our text, we need to split the text up into words. Use the `word_tokenize()` function from the `nltk` library on the text for this and print your results afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1381,
     "status": "ok",
     "timestamp": 1707564009858,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "TgigdyuIHOcI",
    "outputId": "4f408794-0ba1-4276-e3a9-7e16e3c0661c"
   },
   "outputs": [],
   "source": [
    "# import word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# TODO: Split the text saved to your variable text_lower_case into tokens and save them in a tokens list\n",
    "\n",
    "\n",
    "# TODO: Print the words list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWpP4OIvFAY7"
   },
   "source": [
    "**Step 3: Preprocessing - Removing punctuation marks and special characters**\n",
    "\n",
    "Now look at the results: There are still lots of punctuation marks in the text! Think about it: Why might that be a problem?\n",
    "\n",
    "So assume we also want to get rid of all the punctuation marks in the text. How do we do this?\n",
    "The most straightforward way of achieving it is to apply the `isalpha()` to your tokens - it will simply look at all of them and only retain those that only contain alphabetical letters. Try it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1707564030655,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "4x5CDclWFByJ",
    "outputId": "83aad995-73cf-4b15-9c72-52b0c1f3a3dc"
   },
   "outputs": [],
   "source": [
    "# TODO: Initialize an empty list filtered_tokens\n",
    "\n",
    "\n",
    "# Use a for loop to filter alphanumeric tokens by applying the isalpha() function to every token in tokens and then storing the result in your filtered_tokens list\n",
    "for token in tokens:\n",
    "    if token.isalpha():\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "# TODO: print your results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j70o_982SYuu"
   },
   "source": [
    "**Step 4: Preprocessing - Eliminating stop words**\n",
    "\n",
    "Removing stop words in NLP means removing words that don't carry much meaning, such as \"the\" or \"and.\" It helps NLP models focus on the essential words in NLP analysis tasks. Let's remove the stopwords from our token list with the help of the stop word list provided by the `nltk` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1707422474633,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "UOeqxZP_H2Q9",
    "outputId": "6017913e-5a25-444b-ea2b-ae3935ddc8fe"
   },
   "outputs": [],
   "source": [
    "# import nltk library and list of stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# TODO: Initialize an empty filtered_tokens_no_stop list to store filtered words\n",
    "\n",
    "\n",
    "# TODO: Iterate over each word in the words list\n",
    "\n",
    "    # TODO: Check if the word is not a stop word\n",
    "    \n",
    "        # TODO: If it's not a stop word, append the word to the filtered_words list\n",
    "        \n",
    "\n",
    "# TODO: Print your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXRPtxqrTPJa"
   },
   "source": [
    "**Step 5: Preprocessing - Lemmatisation**\n",
    "\n",
    "Lemmatization in NLP entails simplifying words to their base form, reducing variations like \"running\" to \"run.\" It makes analysis more accurate and improves tasks like text comprehension or feature extraction. Let's lemmatise our tokens using the `WordLemmatizer` provided by `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1707422517071,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "6gBCExR5Gj3N",
    "outputId": "e0e8434b-cd64-476b-e249-0826ee7cc91c"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Instantiate the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# TODO: Initialize an empty list to store lemmatized tokens\n",
    "\n",
    "\n",
    "# Use a for loop to lemmatize each token and append the lemmatised token to the lemmatized_tokens list\n",
    "for token in filtered_tokens_no_stop:\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "# TODO: Print the lemmatized tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIaclvxdFCUS"
   },
   "source": [
    "**Step 6: Count the word frequencies**\n",
    "\n",
    "We will now create a Python dictionary to store the word frequencies. A dictionary in Python is similar to a list, but it has the advantage that you can store pairs of items. Each pair has a unique name (key) and a value associated with it. For example, if you encounter the word 'adventure' 8 times in your text, the dictionary would store the unique name (key) 'adventure' together with the number 8 like this:\n",
    "\n",
    "`{'adventure': 8}`\n",
    "\n",
    "For each token from our text, we will update the corresponding entry in the dictionary. If the token is encountered for the first time, a new entry will be added with a frequency of 1. If the word is already present, its frequency will be incremented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1707426803689,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "Y8nREKouuT1n",
    "outputId": "58fe07cc-f8d9-499e-9be2-053557ed6684"
   },
   "outputs": [],
   "source": [
    "# Given a list of words, return a dictionary of word-frequency pairs.\n",
    "\n",
    "# TODO: Define a function that takes a list of words as input\n",
    "\n",
    "    # TODO: Initialise an empty dictionary to save word frequencies\n",
    "    \n",
    "\n",
    "    # TODO: Create a dictionary of word frequencies using a for loop, iterating over every word in the words list\n",
    "    \n",
    "        # TODO: Check if the word is already in the dictionary\n",
    "        \n",
    "            # TODO: Increment the existing count\n",
    "            \n",
    "\n",
    "            # TODO: Add the word to the dictionary with a count of 1\n",
    "            \n",
    "\n",
    "    # TODO: Return the dictionary of word frequencies\n",
    "    \n",
    "\n",
    "# TODO: Call the function with your list of lemmatised tokens and store the result in a variable\n",
    "\n",
    "\n",
    "# TODO: Print the resulting word-frequency dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQO5UyHNcgO-"
   },
   "source": [
    "## Option B: Pre-processing using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIjg6W_W1_nJ"
   },
   "source": [
    "You can also use the tools offered by the spaCy library to pre-process your text. spaCy is a Python library used for natural language processing (NLP). We perform the exact same steps: Tokenisation, lemmatisation, removing punctuation and stopwords, converting the text to lower-case only.\n",
    "\n",
    "Call the `createFreqDict()` function on `input` and save the results in a new variable called `word_freq_dict`. Then print the contents of `word_freq_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8723,
     "status": "ok",
     "timestamp": 1707566560835,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "ABCRQFW5cg2a",
    "outputId": "0f990c5c-4b35-467b-8992-d6e756b95769"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "def createFreqDict(input_text):\n",
    "    # Convert the input text to lowercase\n",
    "    input_text_lower = input_text.lower()\n",
    "\n",
    "    # Load the spaCy language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Tokenize\n",
    "    doc = nlp(input_text_lower)\n",
    "\n",
    "    # Initialize Counter to count word frequencies\n",
    "    word_freq_counter = Counter()\n",
    "\n",
    "    # Process each token in the document\n",
    "    for token in doc:\n",
    "        # Check if the token is a valid word (is_alpha) and not a stop word\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            # Lemmatize the word (get the base form)\n",
    "            lemma = token.lemma_\n",
    "\n",
    "            # Update the Counter with the lemmatized word\n",
    "            word_freq_counter[lemma] += 1\n",
    "\n",
    "    # Convert Counter to dictionary\n",
    "    word_freq_dict = dict(word_freq_counter)\n",
    "\n",
    "    return word_freq_dict\n",
    "\n",
    "# TODO: Call the function with your list of lemmatised tokens and store the result in a variable\n",
    "\n",
    "\n",
    "# TODO: Print the resulting word-frequency dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpUyX6JcbRxi"
   },
   "source": [
    "## Option C: Pre-processing using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqVvYiF131wJ"
   },
   "source": [
    "Another way to pre-process your text is to use the `CountVectorizer()` function provided by the `sklearn` library. It converts a collection of text documents to a matrix of token counts. Apply the `get_word_frequencies()` function to the `input` variable and save the results in a new variable `word_freq_dict`. Then, print the results of `word_freq_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1707567910271,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "scL9gEtnbtrf",
    "outputId": "c1e3cbbc-f0f5-4265-c82c-e618e66ff61a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def get_word_frequencies(input_text):\n",
    "    # Create an instance of CountVectorizer with stop words removed\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform([input_text])\n",
    "\n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Convert the sparse matrix to a dense array\n",
    "    array = X.toarray()\n",
    "\n",
    "    # Create a dictionary of word frequencies\n",
    "    word_freq_dict = dict(zip(feature_names, array[0]))\n",
    "\n",
    "    return word_freq_dict\n",
    "\n",
    "# TODO: Apply the get_word_frequencies() function to your input text and save it in a variable word_freq_dict\n",
    "\n",
    "\n",
    "# TODO: Print the variable word_freq_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27lqrF4DKs_a"
   },
   "source": [
    "## Visualising your data with Matplotlib\n",
    "\n",
    "Now, let's plot our word frequency data using Matplotlib. For this, we will create a bar chart. Call the `plot_word_frequencies()` function with our `word_freq_dict` as input parameter. You should also pass the parameter `top_n=10` to get only the 10 highest word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1707567998209,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "H27eCYvFuF1w",
    "outputId": "16b4c346-1d7d-4338-f3a8-7d64f4566a33"
   },
   "outputs": [],
   "source": [
    "# Import the 'matplotlib.pyplot' module and alias it as 'plt'\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "\n",
    "# Define a function named 'plot_word_frequencies' that takes a 'word_freq_dict' and 'top_n' as inputs\n",
    "def plot_word_frequencies(word_freq_dict, top_n=None):\n",
    "    # Create a new figure for the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Sort the word frequency dictionary by values in descending order\n",
    "    sorted_word_freq = sorted(word_freq_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    # If top_n is provided, select the top N items; otherwise, use all items\n",
    "    sorted_word_freq = sorted_word_freq[:top_n] if top_n is not None else sorted_word_freq\n",
    "\n",
    "    # Unpack the sorted items into separate lists of words and frequencies\n",
    "    words, frequencies = zip(*sorted_word_freq)\n",
    "\n",
    "    # Create a bar plot using the top N words and their frequencies\n",
    "    plt.bar(words, frequencies)\n",
    "\n",
    "    # Label the x-axis as 'Words'\n",
    "    plt.xlabel('Words')\n",
    "\n",
    "    # Label the y-axis as 'Frequencies'\n",
    "    plt.ylabel('Frequencies')\n",
    "\n",
    "    # Set the title of the plot to 'Word Frequencies'\n",
    "    plt.title('Word Frequencies')\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Call the function plot_word_frequencies on the word frequency dictionary as input parameter and plot the top 10 highest frequencies\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMu7xAWGOI4bKlyGC7lBl7s",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
