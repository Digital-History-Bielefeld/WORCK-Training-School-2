{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_anr6_0qMq1"
   },
   "source": [
    "## Looking at words in context: An introduction to n-Grams\n",
    "\n",
    "In natural language processing, examining words individually might not capture the full context of a text. Some words only reveal their complete real-world meaning when you look at them in combination with the previous and the following words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QVHY4BhlLuP"
   },
   "source": [
    "## A real-world example\n",
    "\n",
    "Take a look at the following sentence:\n",
    "\n",
    "\"George Washington, the first President of the United States of America and one of the Founding Fathers, was born in Westmoreland County, Virginia.\"\n",
    "\n",
    "Which words only make sense in context? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOVcoex3lTRE"
   },
   "source": [
    "## What is an n-gram?\n",
    "\n",
    "How can we use our NLP tools to not just give us all the individual words within a text, but to look at the neighbourhood of a word? This is where the concept of n-grams comes into play. The 'n' in n-grams stands for the number of words you want to look at in a given text. The most important information is often in the context immediately before or after a word, so it makes most sense to look at n-grams here.\n",
    "\n",
    "A bigram (2-gram) consists of two words. An example of this would be \"George Washington\" or \"first president\".\n",
    "\n",
    "A trigram (3-gram) consists of three words. An example of this would be \"the Founding Fathers\" and \"the United States\".\n",
    "\n",
    "Keep in mind that n-grams are not necessarily grammatically finite structures: They just provide an alternative way of viewing words in context which may allow for a more meaningful interpretation than you would get when viewing the words in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjbnnsetrgK7"
   },
   "source": [
    "## Looking for n-grams in a text\n",
    "\n",
    "But how do we get all of the n-grams in a text? A systematic way of doing this is just to imagine that you have a window sliding along your text, always one step at a time. The size of the window is always n (the number of words you want to look at).\n",
    "\n",
    "So if you want to have all the bigrams in a text, you would slide along your text like this:\n",
    "\n",
    "![Sliding window bigrams](data/bigrams.png)\n",
    "\n",
    "\n",
    "If you want to know all the trigrams in your text, you would slide along your text like this:\n",
    "\n",
    "![Sliding window trigrams](data/trigrams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtKEwRpCHChs"
   },
   "source": [
    "## Using Python to look for n-grams\n",
    "\n",
    "We're now going to use spaCy and NLTK (Natural Language Toolkit) to tokenize out sample text. Then, we extract bigrams and trigrams from the tokenized sentences and visualise n-grams using the displaCy visualisation tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you get started ...\n",
    "\n",
    "Please run the code below. It will import some necessary libaries. This might take some time ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the libraries listed in thre requirements.txt file\n",
    "%pip install -r ../.devcontainer/python-3.12/requirements.txt --upgrade-strategy only-if-needed\n",
    "\n",
    "# install spacy model en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0oT002NJd9v"
   },
   "source": [
    "**Step 1: Import necessary libraries**\n",
    "\n",
    "For this tutorial, we will need the spaCy and displacy libraries. Let's run the code below to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11432,
     "status": "ok",
     "timestamp": 1707576959800,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "GouRDApmze8k"
   },
   "outputs": [],
   "source": [
    "# import spacy and displacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OE6_D46VKhEZ"
   },
   "source": [
    "**Step 2: Define the text**\n",
    "\n",
    "Next, save our sample text from above into a variable called text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1707576997566,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "tDLWj9-zKt8d"
   },
   "outputs": [],
   "source": [
    "# TODO: save sample text into variable called text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_DsLBMoH_sb"
   },
   "source": [
    "**Step 3: Tokenization**\n",
    "\n",
    "Now we need to tokenize the input text using spaCy. This involves breaking the text into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1707578987686,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "kQolev3oqGx1"
   },
   "outputs": [],
   "source": [
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# TODO: Initialize an empty list called tokens to store the tokens\n",
    "\n",
    "\n",
    "# Extract token texts using a for loop\n",
    "for token in doc:\n",
    "  # Check if the token is alphabetic (not punctuation)\n",
    "  if token.is_alpha:\n",
    "    tokens.append(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQpEP1AyIiTB"
   },
   "source": [
    "Let's print our tokens to see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1707578993013,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "OiHREOO-qYJI",
    "outputId": "972bd1a6-a450-43ab-afbc-a16d30fe1e5e"
   },
   "outputs": [],
   "source": [
    "# TODO: Print the tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSmwETf1Lc5N"
   },
   "source": [
    "**Step 4: Extracting and printing ngrams**\n",
    "\n",
    "Take a look at the following function and try to understand what it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1707579002723,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "wbvZE2LKGeMp"
   },
   "outputs": [],
   "source": [
    "# Function to extract n-grams dynamically\n",
    "def extract_ngrams(tokens, n):\n",
    "    ngrams_list = []\n",
    "    for index in range(len(tokens) - n + 1):\n",
    "        ngram = tuple(tokens[index:index + n])\n",
    "        ngrams_list.append(ngram)\n",
    "    return ngrams_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zopNwz9FGsYr"
   },
   "source": [
    "We will now use this function to extract all bigrams. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1707579043566,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "i9BtcTC-ytNk",
    "outputId": "e046870b-e802-4e3f-8142-d136f210dbd1"
   },
   "outputs": [],
   "source": [
    "# extract bigrams\n",
    "bigrams = extract_ngrams(tokens, 2)\n",
    "\n",
    "# print bigrams\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ukg2fiOHaXK"
   },
   "source": [
    "Do the same for trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1707579050774,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "DisveAXMHeyg",
    "outputId": "f6f3d0be-b949-4751-f98f-1fb5bb8d638a"
   },
   "outputs": [],
   "source": [
    "# TODO: extract trigrams\n",
    "\n",
    "\n",
    "# TODO: print trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaPl-BqrL9mX"
   },
   "source": [
    "**Step 5: Using displaCy to display ngrams**\n",
    "\n",
    "Of course, it would be tedious for us to analyse all the possible combinations of words in context to find out which n-grams make semantic sense in the context of our text. Luckily, displaCy's entity representation can help us out again here. Run the code - which types of meaningful n-grams can you spot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1707579475250,
     "user": {
      "displayName": "Dana Meyer",
      "userId": "07634428609531147649"
     },
     "user_tz": -60
    },
    "id": "ouigJHRUzDRn",
    "outputId": "394822fa-3faa-4bf2-c6f8-4f228e2786cc"
   },
   "outputs": [],
   "source": [
    "# Visualize named entities with displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPKX01ZSIsWzb6jAwwZlhdE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
