{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5258223c",
   "metadata": {},
   "source": [
    "## Â Preprocessing with RegEx and NLTK\n",
    "\n",
    "Here we compare possible preprocessing methods.  \n",
    "\n",
    "**Regular Expression (RegEx)** can be used in Python with the standard library `re`. You can define the patterns of the string you want to have represented/found. A nice RegEx playground can be found here: https://regexr.com\n",
    "\n",
    "The **Natural Language Toolkit (NLTK)** [nltk.org](https://www.nltk.org) is a well known, tried and tested Python package to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be638ab-6c57-4369-8cf0-2897e629ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex\n",
    "import re\n",
    "\n",
    "# import nltk and its tokenizers\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e89040-5b2e-4c20-a400-c2d13a8cc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change the text or add your own\n",
    "text = '''The representatives of many countries met at the United Nations\n",
    "conference in New York. UN general secretary AntÃ³nio Guterres held a speech\n",
    "to open the event; he was focusing on the importance of human rights.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d7784f",
   "metadata": {},
   "source": [
    "Look how the text is represented in the output (e.g. the newlines `\\n`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199104e-e5c3-487a-84d0-8fe22ecb652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91facfd-f4bf-436f-bc51-d6bcdc8620f0",
   "metadata": {},
   "source": [
    "## Removing the newlines (\\n) with re.sub\n",
    "\n",
    "`re.sub(pattern, repl, string)` is the re-package tool to substitute the found patterns with the replacement string you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44d27a8-64c7-4420-8c20-7b3d598f4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = ___\n",
    "replacement = ___\n",
    "re.sub(pattern, replacement, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778d04e-990d-4759-8ec6-a0eaf95b4c5d",
   "metadata": {},
   "source": [
    "## Splitting with re.split()\n",
    "\n",
    "Let's try sentence-tokenize the text manually with `re.split(pattern, string)`. The delimiters are splitted seperatly and kept in the result-list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42415435-78c6-466d-a283-9968bf95e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = ___\n",
    "sents_re = re.split(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5e00b-75ea-4770-bbe4-0c777069f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the sentences, does it make sense?\n",
    "\n",
    "for s in sents_re:\n",
    "    print(s)\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33cfabc",
   "metadata": {},
   "source": [
    "## Splitting with the NLTK sent_tokenizer\n",
    "\n",
    "Now let's split the sentences automatically with the `sent_tokenizer` from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb52c6-fd2a-401f-b1c0-d18d126e7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the imported function to tokenize the sentences\n",
    "sents_nltk = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3b43c-b01c-44f4-83a3-569717e0cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sents_nltk:\n",
    "    print(s)\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce7575-f5c1-4c84-92f5-f50a8c43e0ae",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Since we splitted the sentences and have them in `sents_nltk`, we'll try to tokenize the first sentence with `re.findall(pattern, string)` and `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = ___\n",
    "tokens_re = re.findall(pattern, sents_nltk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f02828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you like it?\n",
    "for t in tokens_re:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2026de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only of the first sentence\n",
    "tokens_nltk = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaacfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokens_nltk:\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb89c19",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "It's gonna be hard to lemmatize the text with the `re` package - this would be a huge pattern. ðŸ˜„\n",
    "We gonna use one of NLTK lemmatizers for the English language, the `WordNetLemmatizer`.  \n",
    "\n",
    "For German there is for example the **GermaNLTK** from the Hochschule der Medien Stuttgart: https://docs.google.com/document/d/1rdn0hOnJNcOBWEZgipdDfSyjJdnv_sinuAUSDSpiQns  \n",
    "\n",
    "It integrates the [GermaNet of the UniversitÃ¤t TÃ¼bingen](https://uni-tuebingen.de/fakultaeten/philosophische-fakultaet/fachbereiche/neuphilologie/seminar-fuer-sprachwissenschaft/arbeitsbereiche/allg-sprachwissenschaft-computerlinguistik/ressourcen/lexica/germanet-1/) in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c16524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bec8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the lemmatization without and with casing\n",
    "for t in tokens_nltk:\n",
    "  t_low = ___\n",
    "  t_cased = ___\n",
    "\n",
    "  print(t +':\\n')\n",
    "  print(t_low)\n",
    "  print(t_cased)\n",
    "  print('-'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931db0b3",
   "metadata": {},
   "source": [
    "## Stopword removal\n",
    "\n",
    "The NLTK package has good stopword lists of 29 languages.  \n",
    "We'll use it to remove the stopwords from our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(stopwords.words('german')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take care: the stopword list is lowercased!\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "kept = []\n",
    "removed = []\n",
    "\n",
    "#Â check if the token t is a stopword\n",
    "# if yes, append it to removed - else to kept\n",
    "for t in tokens_nltk:\n",
    "  ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9747fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(removed)\n",
    "print('*'*30)\n",
    "print(kept)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
